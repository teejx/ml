{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install hdbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Load reduced features\n",
    "FEATURE_FILES = [\"pca.npy\", \"umap.npy\", \"autoencoder.npy\"]\n",
    "\n",
    "features_list = [np.load(f) for f in FEATURE_FILES]\n",
    "features = np.concatenate(features_list, axis=1)\n",
    "features = StandardScaler().fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Optimal Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.datasets import make_blobs  # For sample data (replace with your data)\n",
    "\n",
    "# Generate some sample data (replace with your actual data)\n",
    "your_data, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
    "\n",
    "\n",
    "# 1. MiniBatchKMeans\n",
    "silhouette_scores_mbkmeans = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(your_data)\n",
    "    labels = kmeans.labels_\n",
    "    score = silhouette_score(your_data, labels)\n",
    "    silhouette_scores_mbkmeans.append(score)\n",
    "\n",
    "plt.plot(range(2, 11), silhouette_scores_mbkmeans)\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Method for MiniBatchKMeans\")\n",
    "plt.show()\n",
    "\n",
    "# 2. KMeans (for comparison - MiniBatchKMeans is often used for larger datasets)\n",
    "silhouette_scores_kmeans = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(your_data)\n",
    "    labels = kmeans.labels_\n",
    "    score = silhouette_score(your_data, labels)\n",
    "    silhouette_scores_kmeans.append(score)\n",
    "\n",
    "plt.plot(range(2, 11), silhouette_scores_kmeans)\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Method for KMeans\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 3. Gaussian Mixture Models (GMM)\n",
    "bics = []\n",
    "aics = []\n",
    "for n_components in range(2, 11):\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "    gmm.fit(your_data)\n",
    "    bics.append(gmm.bic(your_data))\n",
    "    aics.append(gmm.aic(your_data))\n",
    "\n",
    "plt.plot(range(2, 11), bics, label=\"BIC\")\n",
    "plt.plot(range(2, 11), aics, label=\"AIC\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Information Criterion\")\n",
    "plt.title(\"BIC and AIC for GMM\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 4. Hierarchical Clustering (AgglomerativeClustering)\n",
    "silhouette_scores_hierarchical = []\n",
    "davies_bouldin_scores_hierarchical = []\n",
    "for k in range(2, 11):\n",
    "    hierarchical = AgglomerativeClustering(n_clusters=k)\n",
    "    labels = hierarchical.fit_predict(your_data)\n",
    "    score = silhouette_score(your_data, labels)\n",
    "    davies_bouldin = davies_bouldin_score(your_data, labels)\n",
    "    silhouette_scores_hierarchical.append(score)\n",
    "    davies_bouldin_scores_hierarchical.append(davies_bouldin)\n",
    "\n",
    "\n",
    "plt.plot(range(2, 11), silhouette_scores_hierarchical, label='Silhouette')\n",
    "plt.plot(range(2, 11), davies_bouldin_scores_hierarchical, label='Davies-Bouldin')\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Silhouette and Davies-Bouldin for Hierarchical Clustering\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 5. DBSCAN (requires different approach for parameter tuning)\n",
    "# Example: Varying epsilon and min_samples\n",
    "eps_values = np.arange(0.5, 2.0, 0.1)  # Example range for epsilon\n",
    "min_samples_values = range(2, 6)  # Example range for min_samples\n",
    "best_score = -1\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(your_data)\n",
    "        # DBSCAN may identify all points as noise (-1), if so, skip score calculation\n",
    "        if len(set(labels)) > 1:\n",
    "            score = silhouette_score(your_data, labels)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_eps = eps\n",
    "                best_min_samples = min_samples\n",
    "\n",
    "print(f\"Best DBSCAN parameters: eps={best_eps}, min_samples={best_min_samples}, Silhouette Score={best_score}\")\n",
    "\n",
    "\n",
    "# Note:  Replace `your_data` with your actual data.  The sample data is just for demonstration.\n",
    "# Also, adjust the parameter ranges (e.g., k values, eps range, min_samples range) as needed for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MiniBatchKmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = MiniBatchKMeans(n_clusters=2, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(features)\n",
    "\n",
    "# Compute silhouette score\n",
    "sil_score = silhouette_score(features, kmeans_labels)\n",
    "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
    "\n",
    "np.save(\"cluster_kmeans.npy\", kmeans_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=features[:, 0], y=features[:, 1], hue=kmeans_labels, palette=\"viridis\", legend=None)\n",
    "plt.title(\"Mini-Batch K-Means Clustering\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gausian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "gmm_labels = gmm.fit_predict(features)\n",
    "\n",
    "sil_score = silhouette_score(features, gmm_labels)\n",
    "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
    "\n",
    "np.save(\"cluster_gmm.npy\", gmm_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=features[:, 0], y=features[:, 1], hue=gmm_labels, palette=\"viridis\", legend=None)\n",
    "plt.title(\"GMM Clustering\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical = AgglomerativeClustering(n_clusters=2)\n",
    "hierarchical_labels = hierarchical.fit_predict(features)\n",
    "\n",
    "sil_score = silhouette_score(features, hierarchical_labels)\n",
    "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
    "\n",
    "np.save(\"cluster_hierarchical.npy\", hierarchical_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=features[:, 0], y=features[:, 1], hue=hierarchical_labels, palette=\"viridis\", legend=None)\n",
    "plt.title(\"Hierarchical Clustering\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = hdbscan.HDBSCAN(min_cluster_size=100)\n",
    "dbscan_labels = dbscan.fit_predict(features)\n",
    "\n",
    "if len(set(dbscan_labels)) > 1:\n",
    "    sil_score = silhouette_score(features, labels)\n",
    "    print(f\"Silhouette Score: {sil_score:.4f}\")\n",
    "else:\n",
    "    print(\"DBSCAN found only one cluster, silhouette score not applicable.\")\n",
    "\n",
    "np.save(\"cluster_dbscan.npy\", dbscan_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=features[:, 0], y=features[:, 1], hue=dbscan_labels, palette=\"viridis\", legend=None)\n",
    "plt.title(\"DBSCAN Clustering\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Clustering completed and results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
